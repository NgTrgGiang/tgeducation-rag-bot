"""
ingest.py - ƒê·ªçc knowledge base JSON ‚Üí t·∫°o embeddings ‚Üí l∆∞u v√†o ChromaDB

Ch·∫°y: python ingest.py
"""
import json
import time
import chromadb
from sentence_transformers import SentenceTransformer
from config import EMBEDDING_MODEL, CHROMA_PERSIST_DIR, COLLECTION_NAME, KB_FILE


def load_knowledge_base(filepath: str) -> list[dict]:
    """ƒê·ªçc file JSON knowledge base."""
    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)
    print(f"üìÑ ƒê√£ ƒë·ªçc {len(data)} entries t·ª´ {filepath}")
    return data


def build_document_text(entry: dict) -> str:
    """
    T·∫°o text t·ªëi ∆∞u cho embedding t·ª´ m·ªôt entry.
    K·∫øt h·ª£p nhi·ªÅu tr∆∞·ªùng ƒë·ªÉ tƒÉng kh·∫£ nƒÉng retrieval.
    """
    parts = []

    # Title - quan tr·ªçng nh·∫•t cho semantic search
    parts.append(f"Ti√™u ƒë·ªÅ: {entry['title']}")

    # Content - n·ªôi dung ch√≠nh
    parts.append(f"N·ªôi dung: {entry['content']}")

    # Summary - t√≥m t·∫Øt gi√∫p embedding hi·ªÉu t·ªïng quan
    parts.append(f"T√≥m t·∫Øt: {entry['summary']}")

    # Typical questions - C·ª∞C K·ª≤ QUAN TR·ªåNG cho RAG
    # Gi√∫p match c√¢u h·ªèi user v·ªõi c√¢u h·ªèi m·∫´u
    if entry.get("typical_questions"):
        questions_text = " | ".join(entry["typical_questions"])
        parts.append(f"C√¢u h·ªèi th∆∞·ªùng g·∫∑p: {questions_text}")

    # Tags - t·ª´ kh√≥a b·ªï sung
    if entry.get("tags"):
        parts.append(f"T·ª´ kh√≥a: {', '.join(entry['tags'])}")

    return "\n".join(parts)


def build_metadata(entry: dict) -> dict:
    """
    T·∫°o metadata cho ChromaDB filtering.
    ChromaDB ch·ªâ h·ªó tr·ª£ str, int, float, bool.
    """
    return {
        "id": entry["id"],
        "title": entry["title"],
        "category": entry["category"],
        "service": entry["service"],
        "student_level": entry["student_level"],
        "subject": entry["subject"],
        "intent": entry["intent"],
        "audience": entry["audience"],
        "priority": entry["priority"],
        "sensitivity": entry["sensitivity"],
        "source_type": entry["source_type"],
        "locale": entry["locale"],
        "escalation_required": entry["escalation_required"],
        "human_handoff_hint": entry.get("human_handoff_hint", ""),
        # L∆∞u summary ri√™ng ƒë·ªÉ hi·ªÉn th·ªã nhanh
        "summary": entry["summary"],
        # L∆∞u content g·ªëc ƒë·ªÉ tr·∫£ v·ªÅ cho LLM
        "content": entry["content"],
    }


def ingest():
    """Main ingestion pipeline."""
    print("=" * 60)
    print("üöÄ TG Education RAG - Knowledge Base Ingestion")
    print("=" * 60)

    # 1. Load knowledge base
    entries = load_knowledge_base(KB_FILE)

    # 2. Initialize embedding model
    print(f"\n‚è≥ ƒêang t·∫£i embedding model: {EMBEDDING_MODEL}...")
    start = time.time()
    model = SentenceTransformer(EMBEDDING_MODEL)
    print(f"‚úÖ Model loaded trong {time.time()-start:.1f}s")

    # 3. Build documents for embedding
    print("\nüìù ƒêang x√¢y d·ª±ng documents cho embedding...")
    documents = []
    metadatas = []
    ids = []

    for entry in entries:
        doc_text = build_document_text(entry)
        metadata = build_metadata(entry)
        documents.append(doc_text)
        metadatas.append(metadata)
        ids.append(entry["id"])

    # 4. Generate embeddings
    print(f"\n‚è≥ ƒêang t·∫°o embeddings cho {len(documents)} documents...")
    start = time.time()
    embeddings = model.encode(documents, show_progress_bar=True, batch_size=8)
    embeddings_list = embeddings.tolist()
    print(f"‚úÖ Embeddings created trong {time.time()-start:.1f}s")
    print(f"   Embedding dimension: {len(embeddings_list[0])}")

    # 5. Store in ChromaDB
    print(f"\nüíæ ƒêang l∆∞u v√†o ChromaDB t·∫°i {CHROMA_PERSIST_DIR}...")
    client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)

    # X√≥a collection c≈© n·∫øu t·ªìn t·∫°i (re-ingest)
    try:
        client.delete_collection(COLLECTION_NAME)
        print(f"   ƒê√£ x√≥a collection c≈© '{COLLECTION_NAME}'")
    except Exception:
        pass

    collection = client.create_collection(
        name=COLLECTION_NAME,
        metadata={"description": "TG Education K12 Customer Support Knowledge Base"}
    )

    # Add documents in batches
    batch_size = 20
    for i in range(0, len(documents), batch_size):
        end = min(i + batch_size, len(documents))
        collection.add(
            ids=ids[i:end],
            documents=documents[i:end],
            embeddings=embeddings_list[i:end],
            metadatas=metadatas[i:end],
        )
        print(f"   ƒê√£ th√™m batch {i//batch_size + 1}: entries {i+1}-{end}")

    # 6. Verify
    count = collection.count()
    print(f"\n{'=' * 60}")
    print(f"‚úÖ HO√ÄN T·∫§T! ƒê√£ ingest {count} documents v√†o ChromaDB")
    print(f"   Collection: {COLLECTION_NAME}")
    print(f"   Persist dir: {CHROMA_PERSIST_DIR}")
    print(f"{'=' * 60}")

    # Quick test
    print("\nüîç Quick test - t√¨m ki·∫øm 'h·ªçc ph√≠ bao nhi√™u'...")
    test_query = "h·ªçc ph√≠ bao nhi√™u"
    test_embedding = model.encode([test_query]).tolist()
    results = collection.query(
        query_embeddings=test_embedding,
        n_results=3,
    )
    print(f"   Top 3 k·∫øt qu·∫£:")
    for i, (doc_id, distance) in enumerate(zip(results["ids"][0], results["distances"][0])):
        meta = results["metadatas"][0][i]
        print(f"   {i+1}. [{doc_id}] {meta['title']} (distance: {distance:.4f})")


if __name__ == "__main__":
    ingest()
